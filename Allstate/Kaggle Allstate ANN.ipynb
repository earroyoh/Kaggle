{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 130)\n",
      "(125546, 130)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "#from sklearn.cross_validation import KFold\n",
    "#from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Read train and test files\n",
    "\n",
    "train = pd.read_csv('input/train.csv')\n",
    "#Drop the first column 'id' since it just has serial numbers. Not useful in the prediction process.\n",
    "#X_train = X_train.iloc[:,1:]\n",
    "X_train = train.drop(['loss', 'id'], 1)\n",
    "\n",
    "test = pd.read_csv('input/test.csv')\n",
    "ids = test['id']\n",
    "X_test = test.drop(['id'], 1)\n",
    "y_train = train['loss']\n",
    "\n",
    "# Size of the dataframe\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "del train\n",
    "del test\n",
    "\n",
    "# We can see that there are 188318 training instances having 130 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9',\n",
      "       'cat10',\n",
      "       ...\n",
      "       'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11',\n",
      "       'cont12', 'cont13', 'cont14'],\n",
      "      dtype='object', length=130)\n",
      "(188318, 1190)\n"
     ]
    }
   ],
   "source": [
    "#cat1 to cat116 have strings. The ML algorithms we are going to study require numberical data\n",
    "#One-hot encoding converts an attribute to a binary vector\n",
    "split=116\n",
    "\n",
    "#Variable to hold the list of variables for an attribute in the train and test data\n",
    "labels = []\n",
    "\n",
    "#get the names of all the columns\n",
    "cols=X_train.columns\n",
    "\n",
    "print(cols)\n",
    "\n",
    "for i in range(0,split):\n",
    "    train = X_train[cols[i]].unique()\n",
    "    test = X_test[cols[i]].unique()\n",
    "    labels.append(list(set(train) | set(test)))    \n",
    "\n",
    "del train\n",
    "del test\n",
    "\n",
    "#Import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#One hot encode all categorical attributes\n",
    "cats = []\n",
    "for i in range(0, split):\n",
    "    #Label encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels[i])\n",
    "    feature = label_encoder.transform(X_train.iloc[:,i])\n",
    "    feature = feature.reshape(X_train.shape[0], 1)\n",
    "    #One hot encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
    "    feature = onehot_encoder.fit_transform(feature)\n",
    "    cats.append(feature)\n",
    "\n",
    "\n",
    "# Make a 2D array from a list of 1D arrays\n",
    "# Commented due to not enough memory error\n",
    "#encoded_cats = np.column_stack(cats)\n",
    "\n",
    "# Print the shape of the encoded data\n",
    "#print(encoded_cats.shape)\n",
    "\n",
    "# Concatenate encoded attributes with continuous attributes\n",
    "#X_train_encoded = np.concatenate((encoded_cats,X_train.iloc[:,split:].values),axis=1)\n",
    "X_train_encoded = np.concatenate((np.column_stack(cats),X_train.iloc[:,split:].values),axis=1)\n",
    "\n",
    "print(X_train_encoded.shape)\n",
    "\n",
    "del cats\n",
    "del feature\n",
    "del X_train\n",
    "#del encoded_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit only to the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_encoded)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train = X_train_encoded\n",
    "#del X_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125546, 1190)\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding also for X_test\n",
    "cats = []\n",
    "for i in range(0, split):\n",
    "    #Label encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels[i])\n",
    "    feature = label_encoder.transform(X_test.iloc[:,i])\n",
    "    feature = feature.reshape(X_test.shape[0], 1)\n",
    "    #One hot encode\n",
    "    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n",
    "    feature = onehot_encoder.fit_transform(feature)\n",
    "    cats.append(feature)\n",
    "\n",
    "X_test_encoded = np.concatenate((np.column_stack(cats),X_test.iloc[:,split:].values),axis=1)\n",
    "\n",
    "del cats\n",
    "del feature\n",
    "del X_test\n",
    "print(X_test_encoded.shape)\n",
    "\n",
    "X_test = scaler.transform(X_test_encoded)\n",
    "#X_test = X_test_encoded\n",
    "#del X_test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318,)\n"
     ]
    }
   ],
   "source": [
    "Y_train = np.asarray(y_train) \n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "#       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "#       hidden_layer_sizes=(130), learning_rate='constant',\n",
    "#       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "#       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "#       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
    "#       verbose=False, warm_start=False)\n",
    "\n",
    "mlp = MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "       hidden_layer_sizes=(130), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=300, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
    "       shuffle=True, solver='lbfgs', tol=0.001, validation_fraction=0.1,\n",
    "       verbose=True, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=130, learning_rate='constant',\n",
       "       learning_rate_init=0.01, max_iter=100, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.001, validation_fraction=0.1,\n",
       "       verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.739154993227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "print(mlp.score(X_train, Y_train))\n",
    "#print(confusion_matrix(y_test,predictions))\n",
    "#print(classification_report(y_test,predictions))\n",
    "len(mlp.coefs_)\n",
    "len(mlp.coefs_[0])\n",
    "len(mlp.intercepts_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = ids  \n",
    "submission['loss'] = predictions\n",
    "submission.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
