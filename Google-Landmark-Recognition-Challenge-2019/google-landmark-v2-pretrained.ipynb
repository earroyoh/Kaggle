{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "google-landmark-v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/earroyoh/Kaggle/blob/master/Google-Landmark-Recognition-Challenge-2019/google-landmark-v2-pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXZT2UsyIVe_",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from shutil import copyfile,rmtree\n",
        "\n",
        "os.mkdir('/tmp/google-landmark')\n",
        "os.mkdir('/tmp/google-landmark/train')\n",
        "\n",
        "#rmtree('/tmp/google-landmark/test')\n",
        "os.mkdir('/tmp/google-landmark/test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GrO42T6Etjy",
        "colab_type": "code",
        "outputId": "5ad18b5c-72b3-45d6-cdea-8efd1d1c1c7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Download training images\n",
        "#!pip3 install gdown\n",
        "!gdown https://drive.google.com/uc?id=10yVowvmFjMkY21-DGF2pej_Lbecfqou7 \\\n",
        "  -O /tmp/google-landmark/resized_images.tar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10yVowvmFjMkY21-DGF2pej_Lbecfqou7\n",
            "To: /tmp/google-landmark/resized_images.tar\n",
            "55.3GB [07:26, 124MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mLij6qde6Ox",
        "colab_type": "code",
        "outputId": "90fd7f91-4d0f-4635-fa11-2e1b716827cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "# Download a training file as validation file\n",
        "!wget --no-check-certificate \\\n",
        "     https://s3.amazonaws.com/google-landmark/train/images_400.tar \\\n",
        "    -O /tmp/google-landmark/test/images_400.tar "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-26 11:03:37--  https://s3.amazonaws.com/google-landmark/train/images_400.tar\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.238.253\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.238.253|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1073154048 (1023M) [application/x-tar]\n",
            "Saving to: ‘/tmp/google-landmark/test/images_400.tar’\n",
            "\n",
            "/tmp/google-landmar 100%[===================>]   1023M  57.0MB/s    in 19s     \n",
            "\n",
            "2019-05-26 11:03:56 (54.4 MB/s) - ‘/tmp/google-landmark/test/images_400.tar’ saved [1073154048/1073154048]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9brUxyTpYZHy",
        "colab_type": "text"
      },
      "source": [
        "The following python code will use the OS library to use Operating System libraries, giving you access to the file system, and the zipfile library allowing you to unzip the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMhhz6dPKgWC",
        "colab_type": "code",
        "outputId": "2199b0cc-66fc-40cf-8960-de3c38855efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "# Download CSV datasets with images id's and landmarks id's\n",
        "#!wget --no-check-certificate \\\n",
        "#     https://www.kaggle.com/google/google-landmarks-dataset/downloads/google-landmarks-dataset.zip \\\n",
        "#    -O /tmp/google-landmark/google-landmarks-dataset.zip\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "     https://s3.amazonaws.com/google-landmark/metadata/train.csv \\\n",
        "    -O /tmp/google-landmark/train.csv\n",
        "#!wget --no-check-certificate \\\n",
        "#     https://s3.amazonaws.com/google-landmark/metadata/test.csv \\\n",
        "#    -O /tmp/google-landmark/test.csv\n",
        "\n",
        "  \n",
        "# Extract files\n",
        "#import zipfile\n",
        "\n",
        "#local_zip = '/tmp/google-landmark/google-landmarks-dataset.zip'\n",
        "#zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "#zip_ref.extractall('/tmp/google-landmark')\n",
        "#zip_ref.close()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-26 17:09:56--  https://s3.amazonaws.com/google-landmark/metadata/train.csv\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.110.77\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.110.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 525832518 (501M) [text/csv]\n",
            "Saving to: ‘/tmp/google-landmark/train.csv’\n",
            "\n",
            "/tmp/google-landmar 100%[===================>] 501.47M  70.9MB/s    in 8.5s    \n",
            "\n",
            "2019-05-26 17:10:05 (58.9 MB/s) - ‘/tmp/google-landmark/train.csv’ saved [525832518/525832518]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PLy3pthUS0D2",
        "colab": {}
      },
      "source": [
        "# Extract images tar files\n",
        "\n",
        "# OS system calls\n",
        "#!tar -xvf '/tmp/google-landmark/train/images_000.tar' -C '/tmp/google-landmark/train'\n",
        "#!tar -tvf '/tmp/google-landmark/train/images_50.tar' -C '/tmp/google-landmark/train'\n",
        "#!tar -tvf '/tmp/google-landmark/train/images_100.tar' -C '/tmp/google-landmark/train'\n",
        "#!tar -tvf '/tmp/google-landmark/train/images_150.tar' -C '/tmp/google-landmark/train'\n",
        "#!tar -tvf '/tmp/google-landmark/train/images_200.tar' -C '/tmp/google-landmark/train'\n",
        "#!tar -tvf '/tmp/google-landmark/train/images_250.tar' -C '/tmp/google-landmark/train'\n",
        "#!tar -tvf '/tmp/google-landmark/train/images_300.tar' -C '/tmp/google-landmark/train'\n",
        "#!tar -tvf '/tmp/google-landmark/train/images_350.tar' -C '/tmp/google-landmark/train'\n",
        "#!tar -xvf '/tmp/google-landmark/test/images_400.tar' -C '/tmp/google-landmark/test'\n",
        "\n",
        "\n",
        "import tarfile\n",
        "\n",
        "tar = tarfile.open(\"/tmp/google-landmark/resized_images.tar\")\n",
        "tar.extractall('/tmp/google-landmark/')\n",
        "tar.close()\n",
        "\n",
        "#tar = tarfile.open(\"/tmp/google-landmark/test/images_400.tar\")\n",
        "#tar.extractall('/tmp/google-landmark/test')\n",
        "#tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o-qUPyfO7Qr8"
      },
      "source": [
        "The contents of the .zip are extracted to the base directory `/tmp/google-landmark`, which in turn each contain `train` and `test` subdirectories.\n",
        "\n",
        "In short: The training set is the data that is used to tell the neural network model that 'this is what a horse looks like', 'this is what a human looks like' etc. \n",
        "\n",
        "One thing to pay attention to in this sample: We do not explicitly label the images as horses or humans. If you remember with the handwriting example earlier, we had labelled 'this is a 1', 'this is a 7' etc.  Later you'll see something called an ImageGenerator being used -- and this is coded to read images from subdirectories, and automatically label them from the name of that subdirectory. So, for example, you will have a 'training' directory containing a 'horses' directory and a 'humans' one. ImageGenerator will label the images appropriately for you, reducing a coding step. \n",
        "\n",
        "Let's define each of these directories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NR_M9nWN-K8B",
        "colab": {}
      },
      "source": [
        "# Directory with our training pictures\n",
        "train_dir = os.path.join('/tmp/google-landmark/train')\n",
        "\n",
        "# Directory with our validation pictures\n",
        "validation_dir = os.path.join('/tmp/google-landmark/test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xcTMKw3b-9g",
        "colab_type": "code",
        "outputId": "51812215-25b8-4bfc-9e76-89637c0653ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create dataframes for ImageDataGenerators flow_from_dataframe\n",
        "\n",
        "train_df = pd.read_csv(r\"/tmp/google-landmark/train.csv\", dtype=str)\n",
        "validation_df = pd.read_csv(r\"/tmp/google-landmark/test.csv\", dtype=str)\n",
        "\n",
        "# Data cleaning, remove space, None url's and then drop url column\n",
        "train_df['id'] = train_df['id'].str.strip()\n",
        "train_df = train_df[train_df['url'] != \"None\"]\n",
        "train_df = train_df.drop(columns=['url'])\n",
        "validation_df['id'] = validation_df['id'].str.strip()\n",
        "#validation_df = validation_df[validation_df['url'] != \"None\"]\n",
        "#validation_df = validation_df.drop(columns=['url'])\n",
        "\n",
        "# Obtain number of classes to create model's sotfmax layer later\n",
        "landmarks = train_df.groupby(\"landmark_id\").count()\n",
        "max_classes = landmarks.count()['id']\n",
        "max_classes\n",
        "#landmarks[:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203094"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8EAIV5HKL1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(validation_df)):\n",
        "  output = \"/tmp/google-landmark/test/\" + validation_df['id'][i] + \".jpg\"\n",
        "  url = validation_df['url'][i]\n",
        "  command = \"wget --no-check-certificate \" + url + \"-O \" + output\n",
        "  !command"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtatwrobqBUC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScSnwSpbV4hC",
        "colab_type": "code",
        "outputId": "1d0d6318-4a03-4dac-b5ef-42f717d3bac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Skip if whole training images downloaded\n",
        "#\n",
        "# Temporary restrict dataframes to loaded tar images\n",
        "# due to restriction of disk space in colab environment\n",
        "pretrain_df = train_df\n",
        "temp_df = pd.DataFrame(columns=['id'], dtype=str)\n",
        "for path,dir,file in os.walk('/tmp/google-landmark/train'):\n",
        "  for fn in file:\n",
        "    temp_df = temp_df.append({'id': fn[:-4]}, ignore_index=True)\n",
        "\n",
        "train_df = pretrain_df.loc[pretrain_df['id'].isin(temp_df['id'])]\n",
        "\n",
        "temp_df=pd.DataFrame(columns=['id'], dtype=str)\n",
        "for path,dir,file in os.walk('/tmp/google-landmark/test'):\n",
        "  for fn in file:  \n",
        "    temp_df = temp_df.append({'id': fn[:-4]}, ignore_index=True)\n",
        "\n",
        "validation_df = pretrain_df.loc[pretrain_df['id'].isin(temp_df['id'])]\n",
        "\n",
        "# Try to fix None of Index are in columns error\n",
        "#train_df.reset_index(drop=True)\n",
        "#validation_df.reset_index(drop=True)\n",
        "\n",
        "landmarks = train_df.groupby(\"landmark_id\").count()\n",
        "max_classes = landmarks.count()[\"id\"]\n",
        "max_classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41838"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk3pE6xCdDcl",
        "colab_type": "code",
        "outputId": "184f761e-a3b1-46df-9a61-beb9feda0450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>landmark_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6e158a47eb2ca3f6</td>\n",
              "      <td>142820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>202cd79556f30760</td>\n",
              "      <td>104169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3ad87684c99c06e1</td>\n",
              "      <td>37914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>e7f70e9c61e66af3</td>\n",
              "      <td>102140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4072182eddd0100e</td>\n",
              "      <td>2474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id landmark_id\n",
              "0  6e158a47eb2ca3f6      142820\n",
              "1  202cd79556f30760      104169\n",
              "2  3ad87684c99c06e1       37914\n",
              "3  e7f70e9c61e66af3      102140\n",
              "4  4072182eddd0100e        2474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5oqBkNBJmtUv"
      },
      "source": [
        "## Building a Small Model from Scratch\n",
        "\n",
        "But before we continue, let's start defining the model:\n",
        "\n",
        "Step 1 will be to import tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvfZg3LQbD-5",
        "colab_type": "code",
        "outputId": "b303c040-af05-4efc-c181-1e209e232ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "!pip install --pre -U tensorflow\n",
        "import tensorflow as tf"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/39/f99185d39131b8333afcfe1dcdb0629c2ffc4ecfb0e4c14ca210d620e56c/tensorflow-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (79.9MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9MB 417kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.3)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.7)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 26.7MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.4)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.9)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 25.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow) (0.15.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, google-pasta, tensorflow\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190301 tensorflow-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BnhYCP4tdqjC"
      },
      "source": [
        "We then add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gokG5HKpdtzm",
        "colab_type": "text"
      },
      "source": [
        "Finally we add the densely connected layers. \n",
        "\n",
        "Note that because we are facing a two-class classification problem, i.e. a *binary classification problem*, we will end our network with a [*softmax* activation](https://wikipedia.org/wiki/Sigmoid_function), so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PixZ2s5QbYQ3",
        "colab_type": "code",
        "outputId": "a9b9ec66-aebf-404d-cb73-3524b7eb1754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tensorflow.keras import layers, models, Model\n",
        "#!wget --no-check-certificate \\\n",
        "#    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "#    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  \n",
        "#from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "#local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "#pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n",
        "#                                include_top = False, \n",
        "#                                weights = None)\n",
        "\n",
        "#pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "filepath = '/tmp/google-landmark/GLRC2019-earroyoh.hd5'\n",
        "pre_trained_model = tf.keras.models.load_model(\n",
        "    filepath,\n",
        "    custom_objects=None,\n",
        "    compile=False\n",
        ")\n",
        "\n",
        "for layer in pre_trained_model.layers[:-3]:\n",
        "  layer.trainable = True\n",
        "  \n",
        "# pre_trained_model.summary()\n",
        "\n",
        "#last_layer = pre_trained_model.get_layer('mixed7')\n",
        "last_layer = pre_trained_model.get_layer('dropout_1')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# In case of use of InceptionV3 transfer learning\n",
        "# Flatten the output layer to 1 dimension\n",
        "#x = layers.Flatten()(last_output) # In case of use of InceptionV3 transfer learning\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "#x = layers.Dense(512, activation='relu')(x)\n",
        "# Add a dropout rate of 0.2\n",
        "#x = layers.Dropout(0.2)(x)                  \n",
        "# Add a final softmax layer for classification\n",
        "x = layers.Dense(max_classes, activation='softmax')(last_output)\n",
        "\n",
        "model = Model(pre_trained_model.input, x) \n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s9EaFDP5srBa"
      },
      "source": [
        "model = Model( pre_trained_model.input, x) The model.summary() method call prints a summary of the NN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ZKj8392nbgP",
        "outputId": "0bb67969-7f14-4943-cfeb-99b11d7496f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9145
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 150, 150, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 74, 74, 32)   864         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_94 (Batc (None, 74, 74, 32)   96          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 74, 74, 32)   0           batch_normalization_v1_94[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 72, 72, 32)   9216        activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_95 (Batc (None, 72, 72, 32)   96          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 72, 72, 32)   0           batch_normalization_v1_95[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 72, 72, 64)   18432       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_96 (Batc (None, 72, 72, 64)   192         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 72, 72, 64)   0           batch_normalization_v1_96[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 35, 35, 64)   0           activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 35, 35, 80)   5120        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_97 (Batc (None, 35, 35, 80)   240         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 35, 35, 80)   0           batch_normalization_v1_97[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 33, 33, 192)  138240      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_98 (Batc (None, 33, 33, 192)  576         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 33, 33, 192)  0           batch_normalization_v1_98[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 192)  0           activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 16, 16, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_102 (Bat (None, 16, 16, 64)   192         conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_102[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 16, 16, 48)   9216        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 16, 16, 96)   55296       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_100 (Bat (None, 16, 16, 48)   144         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_103 (Bat (None, 16, 16, 96)   288         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 16, 16, 48)   0           batch_normalization_v1_100[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_103[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 16, 16, 192)  0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 16, 16, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 16, 16, 64)   76800       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 16, 16, 96)   82944       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 16, 16, 32)   6144        average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_99 (Batc (None, 16, 16, 64)   192         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_101 (Bat (None, 16, 16, 64)   192         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_104 (Bat (None, 16, 16, 96)   288         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_105 (Bat (None, 16, 16, 32)   96          conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_99[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_101[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_104[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 16, 16, 32)   0           batch_normalization_v1_105[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 16, 16, 256)  0           activation_99[0][0]              \n",
            "                                                                 activation_101[0][0]             \n",
            "                                                                 activation_104[0][0]             \n",
            "                                                                 activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_109 (Bat (None, 16, 16, 64)   192         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_109[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 16, 16, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 16, 16, 96)   55296       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_107 (Bat (None, 16, 16, 48)   144         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_110 (Bat (None, 16, 16, 96)   288         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 16, 16, 48)   0           batch_normalization_v1_107[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_110[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_10 (AveragePo (None, 16, 16, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 16, 16, 64)   76800       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 16, 16, 96)   82944       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 16, 16, 64)   16384       average_pooling2d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_106 (Bat (None, 16, 16, 64)   192         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_108 (Bat (None, 16, 16, 64)   192         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_111 (Bat (None, 16, 16, 96)   288         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_112 (Bat (None, 16, 16, 64)   192         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_106[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_108[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_111[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_112[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 16, 16, 288)  0           activation_106[0][0]             \n",
            "                                                                 activation_108[0][0]             \n",
            "                                                                 activation_111[0][0]             \n",
            "                                                                 activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_116 (Bat (None, 16, 16, 64)   192         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_116[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 16, 16, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 16, 16, 96)   55296       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_114 (Bat (None, 16, 16, 48)   144         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_117 (Bat (None, 16, 16, 96)   288         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 16, 16, 48)   0           batch_normalization_v1_114[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_117[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_11 (AveragePo (None, 16, 16, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 16, 16, 64)   76800       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 16, 16, 96)   82944       activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 16, 16, 64)   18432       average_pooling2d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_113 (Bat (None, 16, 16, 64)   192         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_115 (Bat (None, 16, 16, 64)   192         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_118 (Bat (None, 16, 16, 96)   288         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_119 (Bat (None, 16, 16, 64)   192         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_113[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_115[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_118[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_119[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 16, 16, 288)  0           activation_113[0][0]             \n",
            "                                                                 activation_115[0][0]             \n",
            "                                                                 activation_118[0][0]             \n",
            "                                                                 activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 16, 16, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_121 (Bat (None, 16, 16, 64)   192         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_121[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 16, 16, 96)   55296       activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_122 (Bat (None, 16, 16, 96)   288         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_122[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 7, 7, 96)     82944       activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_120 (Bat (None, 7, 7, 384)    1152        conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_123 (Bat (None, 7, 7, 96)     288         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 7, 7, 384)    0           batch_normalization_v1_120[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 7, 7, 96)     0           batch_normalization_v1_123[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_120[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_128 (Bat (None, 7, 7, 128)    384         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_128[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 7, 7, 128)    114688      activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_129 (Bat (None, 7, 7, 128)    384         conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_129[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 7, 7, 128)    114688      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_125 (Bat (None, 7, 7, 128)    384         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_130 (Bat (None, 7, 7, 128)    384         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_125[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_130[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 7, 7, 128)    114688      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 7, 7, 128)    114688      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_126 (Bat (None, 7, 7, 128)    384         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_131 (Bat (None, 7, 7, 128)    384         conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_126[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_131[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 7, 7, 192)    172032      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 7, 7, 192)    172032      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_124 (Bat (None, 7, 7, 192)    576         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_127 (Bat (None, 7, 7, 192)    576         conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_132 (Bat (None, 7, 7, 192)    576         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_133 (Bat (None, 7, 7, 192)    576         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_124[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_127[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_132[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_133[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_124[0][0]             \n",
            "                                                                 activation_127[0][0]             \n",
            "                                                                 activation_132[0][0]             \n",
            "                                                                 activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_138 (Bat (None, 7, 7, 160)    480         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_138[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 7, 7, 160)    179200      activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_139 (Bat (None, 7, 7, 160)    480         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_139[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 7, 7, 160)    179200      activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_135 (Bat (None, 7, 7, 160)    480         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_140 (Bat (None, 7, 7, 160)    480         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_135[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_140[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 7, 7, 160)    179200      activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 7, 7, 160)    179200      activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_136 (Bat (None, 7, 7, 160)    480         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_141 (Bat (None, 7, 7, 160)    480         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_136[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_141[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_13 (AveragePo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 7, 7, 192)    215040      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 7, 7, 192)    215040      activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_134 (Bat (None, 7, 7, 192)    576         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_137 (Bat (None, 7, 7, 192)    576         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_142 (Bat (None, 7, 7, 192)    576         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_143 (Bat (None, 7, 7, 192)    576         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_134[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_137[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_142[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_143[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_134[0][0]             \n",
            "                                                                 activation_137[0][0]             \n",
            "                                                                 activation_142[0][0]             \n",
            "                                                                 activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_148 (Bat (None, 7, 7, 160)    480         conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_148[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 7, 7, 160)    179200      activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_149 (Bat (None, 7, 7, 160)    480         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_149[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 7, 7, 160)    179200      activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_145 (Bat (None, 7, 7, 160)    480         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_150 (Bat (None, 7, 7, 160)    480         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_145[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_150[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 7, 7, 160)    179200      activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 7, 7, 160)    179200      activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_146 (Bat (None, 7, 7, 160)    480         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_151 (Bat (None, 7, 7, 160)    480         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_146[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_151[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_14 (AveragePo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 7, 7, 192)    215040      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 7, 7, 192)    215040      activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_144 (Bat (None, 7, 7, 192)    576         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_147 (Bat (None, 7, 7, 192)    576         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_152 (Bat (None, 7, 7, 192)    576         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_153 (Bat (None, 7, 7, 192)    576         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_144[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_147[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_152[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_153[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_144[0][0]             \n",
            "                                                                 activation_147[0][0]             \n",
            "                                                                 activation_152[0][0]             \n",
            "                                                                 activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_158 (Bat (None, 7, 7, 192)    576         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_158[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 7, 7, 192)    258048      activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_159 (Bat (None, 7, 7, 192)    576         conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_159[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 7, 7, 192)    258048      activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_155 (Bat (None, 7, 7, 192)    576         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_160 (Bat (None, 7, 7, 192)    576         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_155[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_160[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 7, 7, 192)    258048      activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 7, 7, 192)    258048      activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_156 (Bat (None, 7, 7, 192)    576         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_161 (Bat (None, 7, 7, 192)    576         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_156[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_161[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_15 (AveragePo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 7, 7, 192)    258048      activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 7, 7, 192)    258048      activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_154 (Bat (None, 7, 7, 192)    576         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_157 (Bat (None, 7, 7, 192)    576         conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_162 (Bat (None, 7, 7, 192)    576         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_163 (Bat (None, 7, 7, 192)    576         conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_154[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_157[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_162[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_163[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_154[0][0]             \n",
            "                                                                 activation_157[0][0]             \n",
            "                                                                 activation_162[0][0]             \n",
            "                                                                 activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 37632)        0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          19268096    flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 203094)       104187222   dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 132,430,582\n",
            "Trainable params: 132,411,766\n",
            "Non-trainable params: 18,816\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DmtkTn06pKxF"
      },
      "source": [
        "The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PEkKSpZlvJXA"
      },
      "source": [
        "Next, we'll configure the specifications for model training. We will train our model with the `binary_crossentropy` loss, because it's a binary classification problem and our final activation is a sigmoid. (For a refresher on loss metrics, see the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/descending-into-ml/video-lecture).) We will use the `rmsprop` optimizer with a learning rate of `0.001`. During training, we will want to monitor classification accuracy.\n",
        "\n",
        "**NOTE**: In this case, using the [RMSprop optimization algorithm](https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) is preferable to [stochastic gradient descent](https://developers.google.com/machine-learning/glossary/#SGD) (SGD), because RMSprop automates learning-rate tuning for us. (Other optimizers, such as [Adam](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) and [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad), also automatically adapt the learning rate during training, and would work equally well here.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8DHWhFP_uhq3",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sn9m9D3UimHM"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Let's set up data generators that will read pictures in our source folders, convert them to `float32` tensors, and feed them (with their labels) to our network. We'll have one generator for the training images and one for the validation images. Our generators will yield batches of images of size 300x300 and their labels (binary).\n",
        "\n",
        "As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network. (It is uncommon to feed raw pixels into a convnet.) In our case, we will preprocess our images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range).\n",
        "\n",
        "In Keras this can be done via the `keras.preprocessing.image.ImageDataGenerator` class using the `rescale` parameter. This `ImageDataGenerator` class allows you to instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`. These generators can then be used with the Keras model methods that accept data generators as inputs: `fit_generator`, `evaluate_generator`, and `predict_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4Imls9zO3Qs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add (path and) jpg extension to id column values\n",
        "def rename_id(id):\n",
        "  #return id[0] + \"/\" + id[1] + \"/\" + id[2] + \"/\" + id + \".jpg\";\n",
        "  #return id + \".jpg\";\n",
        "  return \"train/\" + id + \".jpg\";\n",
        "\n",
        "train_df['id'] = train_df['id'].apply(rename_id)\n",
        "#validation_df['id'] = validation_df['id'].apply(rename_id)\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ClebU9NJg99G",
        "outputId": "7ecb494e-a64d-4176-c6d8-eaa9dda3d5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   #zoom_range = 0.2) # Remove zoom_range augmentation when training the whole dataset          \n",
        "                                   validation_split = 0.2)                        \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe=train_df,\n",
        "        directory=train_dir, # This is the source directory for training images\n",
        "        x_col=\"id\",\n",
        "        y_col=\"landmark_id\",\n",
        "        shuffle=True,\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=512,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "        dataframe=validation_df,\n",
        "        directory=validation_dir,  # This is the source directory for training images\n",
        "        x_col=\"id\",\n",
        "        y_col=\"landmark_id\",\n",
        "        shuffle=True,\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=512,\n",
        "        # Since we use categorical_crossentropy loss, we need categorical labels\n",
        "        class_mode='categorical')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 66128 images belonging to 41838 classes.\n",
            "Found 8266 images belonging to 7483 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mu3Jdwkjwax4"
      },
      "source": [
        "### Training\n",
        "Let's train for 15 epochs -- this may take a few minutes to run.\n",
        "\n",
        "Do note the values per epoch.\n",
        "\n",
        "The Loss and Accuracy are a great indication of progress of training. It's making a guess as to the classification of the training data, and then measuring it against the known label, calculating the result. Accuracy is the portion of correct guesses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fb1_lgobv81m",
        "outputId": "24512efe-b996-4a47-d331-041434067492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
        "#STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=STEP_SIZE_TRAIN,  \n",
        "      epochs=50,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=STEP_SIZE_VALID)\n",
        "#)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 2/64 [..............................] - ETA: 1:18:56 - loss: 10.6175 - acc: 0.0000e+00"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9iUfuU6UPxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WF5y2l4hknb",
        "colab_type": "text"
      },
      "source": [
        "### Plot accuracy\n",
        "\n",
        "Plot training and validation accuracy to check overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1dQ3LUvhViK",
        "colab_type": "code",
        "outputId": "c22650f7-b1e9-4321-919b-af3d83e68357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-8bc21a7567ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_acc'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6vSHzPR2ghH",
        "colab_type": "text"
      },
      "source": [
        "###Running the Model\n",
        "\n",
        "Let's now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, it will then upload them, and run them through the model, giving an indication of whether the object is a horse or a human."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoWp43WxJDNT",
        "colab_type": "code",
        "outputId": "c4565e7f-1ebe-45b3-db41-578f7aca0e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "import numpy as np\n",
        "#from google.colab import files\n",
        "#from tf.keras.preprocessing import image\n",
        "\n",
        "uploaded = os.listdir('/tmp/google-landmark/test/')\n",
        "\n",
        "for fn in uploaded:\n",
        " \n",
        "  # predicting images\n",
        "  path = '/tmp/google-landmark/test/c/c/f/' + fn\n",
        "  img = tf.keras.preprocessing.image.load_img(path, target_size=(150, 150))\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  #print(classes[0])\n",
        "  index_max = np.argmax(classes[0])\n",
        "  print(\"index_max: \" + str(index_max))\n",
        "  print(\"prob: \" + str(classes[0][index_max]))                          \n",
        "  landmarks_value = train_df.sort_values(['landmark_id'], ascending=True).iloc[index_max]['landmark_id']\n",
        "  print(landmarks_value)\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7214dc18-ad44-414e-9b40-250dca511d32\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7214dc18-ad44-414e-9b40-250dca511d32\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-83f6644adb6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: google.colab._files is undefined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-8EHQyWGDvWz"
      },
      "source": [
        "### Visualizing Intermediate Representations\n",
        "\n",
        "To get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\n",
        "\n",
        "Let's pick a random image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-5tES8rXFjux",
        "outputId": "68f394bf-3918-44e1-a5a4-fb6bbc9676e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "#visualization_model = Model(img_input, successive_outputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# Now let's display our representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-410a39ea43f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvisualization_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuccessive_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Let's prepare a random input image from the training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhorse_img_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_horse_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_horse_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mhuman_img_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_human_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_human_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorse_img_files\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhuman_img_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_horse_names' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tuqK2arJL0wo"
      },
      "source": [
        "As you can see we go from the raw pixels of the images to increasingly abstract and compact representations. The representations downstream start highlighting what the network pays attention to, and they show fewer and fewer features being \"activated\"; most are set to zero. This is called \"sparsity.\" Representation sparsity is a key feature of deep learning.\n",
        "\n",
        "\n",
        "These representations carry increasingly less information about the original pixels of the image, but increasingly refined information about the class of the image. You can think of a convnet (or a deep network in general) as an information distillation pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Clean Up\n",
        "\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "651IgjLyo-Jx",
        "colab": {}
      },
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}